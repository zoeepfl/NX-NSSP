{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "*Please name your Example notebook as transparently as possible. Feel free to add a description of the workflow's goal. [Remove this line when creating your example]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Notebook Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author of this template** : Michèle Masson-Trottier [Remove this line when creating your example]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: \n",
    "*Include the name of all authors of the workflow showcased in the example notebook*.\n",
    "\n",
    "**Date**:\n",
    "\n",
    "### Citation and Resources:\n",
    "*Please include the citations for all tools, software, and publications referenced in the workflow. If you are basing your workflow on another previously published example notebook, please cite the name and DOI.*\n",
    "\n",
    "#### Tools included in this workflow\n",
    "*Cite the tools based on their preferred method (example through a Zenodo DOI or a publication)*\n",
    "\n",
    "#### Workflows this work is based on (if applicable)\n",
    "*If you started from another published workflow, please cite it here*\n",
    "\n",
    "#### Publications\n",
    "*Any publication that helped you build your example should be cited here*\n",
    "\n",
    "#### Educational resources\n",
    "*Any educational resource (for example Andy's Brain Book, FSL tutorials...) should be cited here.*\n",
    "\n",
    "#### Dataset\n",
    "*Please cite the database and dataset used in your example*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional:** Add a Table of Contents (ToC) below if it helps navigation during interactive use on Neurodesk. \n",
    "When rendered on GitHub Pages, a sidebar ToC is already available, so an inline ToC may be unnecessary.\n",
    "\n",
    "Each link must match a corresponding section heading: ` ## 1. Data Preparation → #1-Data-Preparation` (keep spaces as hyphens, preserve capital letters), as demonstrated in the next cell.\n",
    "\n",
    "\n",
    "[Remove this cell when creating your example]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of content\n",
    "[1. Data Preparation](#1.-Data-Preparation)  \n",
    "[2. Analysis](#2.-Analysis)  \n",
    "[3. Results](#3.-Results) \n",
    "\n",
    "[Remove this cell if no ToC is used in your notebook]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load software tools and import python libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please have a bloc for loading the different modules needed for your example. If you can not find all the required tools in Neurodesk, feel free to add tools using the web-based GUI https://www.neurodesk.org/neurocontainers-ui/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load FSL 6.0.4\n",
    "import module\n",
    "await module.load('fsl/6.0.4')\n",
    "await module.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please consult the python packages included in the base image [see list here](https://github.com/neurodesk/neurodesktop/blob/main/Dockerfile). If you are using python packages different than these, be sure to `pip install` them in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install pydicom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import pydicom # Library to work with DICOM files\n",
    "import matplotlib.pyplot as plt # Plotting library for displaying images\n",
    "import os # Standard library to interact with the filesystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "*Please indicate clearly where the data is sourced. In this template, there's an example for Datalad and OSF*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our code (not neurodesk template) :\n",
    "# START TO RUN FROM HERE, IGNORE CELLS ABOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gtk-Message: 11:19:58.683: Failed to load module \"canberra-gtk-module\"\n"
     ]
    }
   ],
   "source": [
    "%gui wx\n",
    "import sys\n",
    "import os\n",
    "\n",
    "#####################\n",
    "# Import of utils.py functions\n",
    "#####################\n",
    "# Required to get utils.py and access its functions\n",
    "notebook_dir = os.path.abspath(\"\")\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "sys.path.append('.')\n",
    "from utils import loadFSL, FSLeyesServer, mkdir_no_exist\n",
    "\n",
    "\n",
    "#############################\n",
    "# Loading fsl and freesurfer within Neurodesk\n",
    "# You can find the list of available other modules by clicking on the \"Softwares\" tab on the left\n",
    "#############################\n",
    "import lmod\n",
    "await lmod.purge(force=True)\n",
    "await lmod.load('fsl/6.0.7.4')\n",
    "await lmod.load('freesurfer/7.4.1')\n",
    "await lmod.list()\n",
    "\n",
    "####################\n",
    "# Setup FSL path\n",
    "####################\n",
    "loadFSL()\n",
    "\n",
    "###################\n",
    "# Load all relevant libraries for the lab\n",
    "##################\n",
    "import fsl.wrappers\n",
    "from fsl.wrappers import fslmaths\n",
    "\n",
    "import mne_nirs\n",
    "import nilearn\n",
    "from nilearn.datasets import fetch_development_fmri\n",
    "\n",
    "import mne\n",
    "import mne_nirs\n",
    "import dipy\n",
    "from dipy.data import fetch_bundles_2_subjects, read_bundles_2_subjects\n",
    "import xml.etree.ElementTree as ET\n",
    "import os.path as op\n",
    "import nibabel as nib\n",
    "import glob\n",
    "\n",
    "import ants\n",
    "\n",
    "import openneuro\n",
    "from mne.datasets import sample\n",
    "from mne_bids import BIDSPath, read_raw_bids, print_dir_tree, make_report\n",
    "\n",
    "\n",
    "# Useful imports to define the direct download function below\n",
    "import requests\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# FSL function wrappers which we will call from python directly\n",
    "from fsl.wrappers import fast, bet\n",
    "from fsl.wrappers.misc import fslroi\n",
    "from fsl.wrappers import flirt\n",
    "from fsl.wrappers import mcflirt\n",
    "\n",
    "# General purpose imports to handle paths, files etc\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import subprocess\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "anatomical_path = op.join(\"subject101410\", \"T1w\", \"T1w.nii.gz\") #the original brain\n",
    "subject_id = '101410'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivatives will be written to: /data/derivatives/preprocessed_data/subject_101410\n"
     ]
    }
   ],
   "source": [
    "derivatives = op.join(\"derivatives\", \"preprocessed_data\", \"subject_101410\")\n",
    "deriv_anat = op.join(derivatives, \"anat\")\n",
    "deriv_func = op.join(derivatives, \"func\")\n",
    "\n",
    "for d in (deriv_anat, deriv_func):\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(\"Derivatives will be written to:\", os.path.abspath(derivatives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bids_root   = \"/data/NX-NSSP\"  \n",
    "#preproc_root = op.join(bids_root, 'derivatives','preprocessed_data')\n",
    "#deriv_root = op.join(bids_root, 'derivatives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:20:03: Debug: Adding duplicate image handler for 'Windows bitmap file'\n",
      "11:20:03: Debug: Adding duplicate animation handler for '1' type\n",
      "11:20:03: Debug: Adding duplicate animation handler for '2' type\n",
      "11:20:03: Debug: Adding duplicate image handler for 'Windows bitmap file'\n",
      "11:20:03: Debug: Adding duplicate animation handler for '1' type\n",
      "11:20:03: Debug: Adding duplicate animation handler for '2' type\n",
      "\n",
      "(ipykernel_launcher.py:6063): Gtk-CRITICAL **: 11:20:03.487: gtk_window_resize: assertion 'height > 0' failed\n"
     ]
    }
   ],
   "source": [
    "fsleyesDisplay = FSLeyesServer()\n",
    "fsleyesDisplay.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(ipykernel_launcher.py:6063): Gdk-WARNING **: 11:20:09.802: gdkdrawable-x11.c:952 drawable is not a pixmap or window\n"
     ]
    }
   ],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(anatomical_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skull_stripped_anatomical(deriv_anat, anatomical_path, subject_id, robust=False):\n",
    "    \"\"\"\n",
    "    Function to perform skull-stripping (removing the skull around the brain).\n",
    "    This is a simple wrapper around the brain extraction tool (BET) in FSL's suite\n",
    "    It assumes data to be in the BIDS format (which we will cover in the following weeks).\n",
    "    The method also saves the brain mask which was used to extract the brain.\n",
    "\n",
    "    The brain extraction is conducted only on the T1w of the participant.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    subject_id: string\n",
    "        Subject ID, the subject on which brain extraction should be conducted.\n",
    "    robust: bool\n",
    "        Whether to conduct robust center estimation with BET or not. Default is False.\n",
    "    \"\"\"\n",
    "    # We perform here skull stripping (you'll learn more about it next week!).\n",
    "    # For now all you need to do is that we remove the bones and flesh from the MRI to get the brain!\n",
    "    betted_brain_path = op.join(deriv_anat, 'sub-{}_T1w'.format(subject_id))\n",
    "    os.system('bet {} {} -m {}'.format(anatomical_path, betted_brain_path, '-R' if robust else ''))\n",
    "    print(\"Done with BET.\")\n",
    "\n",
    "resulting_mask_path = op.join(deriv_anat, 'sub-101410_T1w_mask')\n",
    "get_skull_stripped_anatomical(deriv_anat, anatomical_path, subject_id, robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.load(resulting_mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fsl_math_approach(img_path, mask_path, masked_img_path):\n",
    "    os.system('fslmaths {} -mas {} {}'.format(img_path, mask_path, masked_img_path))\n",
    "    \n",
    "betted_brain_path = op.join(deriv_anat, 'sub-101410_T1w.nii.gz') # The brain without skull is in the derivatives folder\n",
    "resulting_mask_path = op.join(deriv_anat, 'sub-101410_T1w_mask.nii.gz') # The mask to use\n",
    "\n",
    "apply_fsl_math_approach(anatomical_path, resulting_mask_path, betted_brain_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(betted_brain_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)b) Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bet_path = op.join(deriv_anat, 'sub-101410_T1w')\n",
    "fast_target = bet_path # Replace with either anatomical_path or bet_path (note: you can try both and decide which is more reasonable!)\n",
    "\n",
    "[os.remove(f) for f in glob.glob(op.join(deriv_anat, '*fast*'))] # Just to clean the directory in between runs of the cell\n",
    "segmentation_path = op.join(deriv_anat, 'sub-101410_T1w_fast')\n",
    "fast(imgs=[fast_target], out=segmentation_path, n_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pve_0 is CSF, pve_1 is grey matter, pve_2 is white matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(bet_path)\n",
    "fsleyesDisplay.load(glob.glob(op.join(deriv_anat,'*pve_0*'))[0])\n",
    "fsleyesDisplay.load(glob.glob(op.join(deriv_anat,'*pve_1*'))[0])\n",
    "fsleyesDisplay.load(glob.glob(op.join(deriv_anat,'*pve_2*'))[0])\n",
    "fsleyesDisplay.displayCtx.getOpts(fsleyesDisplay.overlayList[1]).cmap = 'Red'\n",
    "fsleyesDisplay.displayCtx.getOpts(fsleyesDisplay.overlayList[2]).cmap = 'Green'\n",
    "fsleyesDisplay.displayCtx.getOpts(fsleyesDisplay.overlayList[3]).cmap = 'Blue'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)a) Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functional paths have been created\n"
     ]
    }
   ],
   "source": [
    "# Create functional paths\n",
    "functional_path_LR = op.join(\"subject101410\", \"fMRI\", \"tfMRI_MOTOR_LR\", \"tfMRI_MOTOR_LR.nii\") \n",
    "functional_path_RL = op.join(\"subject101410\", \"fMRI\", \"tfMRI_MOTOR_RL\", \"tfMRI_MOTOR_RL.nii\") \n",
    "\n",
    "print(\"Functional paths have been created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining size in derivatives folder: 953.60 MB\n"
     ]
    }
   ],
   "source": [
    "# Code to verify folder size\n",
    "total_size = sum(os.path.getsize(os.path.join(deriv_func, f)) for f in os.listdir(deriv_func))\n",
    "print(f\"Remaining size in derivatives folder: {total_size / (1024**2):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing variance for LR run\n",
      "Global variance for LR: 519161344.000000\n",
      "Scaling factor for LR: 4.388830677732065e-05\n",
      "Saved normalized LR run: derivatives/preprocessed_data/subject_101410/func/tfMRI_MOTOR_LR_varscaled.nii.gz\n",
      "\n",
      "Normalizing variance for RL run\n",
      "Global variance for RL: 519202016.000000\n",
      "Scaling factor for RL: 4.3886587735293315e-05\n",
      "Saved normalized RL run: derivatives/preprocessed_data/subject_101410/func/tfMRI_MOTOR_RL_varscaled.nii.gz\n",
      "\n",
      "Concatenating normalized runs...\n",
      "Done! Final concatenated file: derivatives/preprocessed_data/subject_101410/func/sub-101410_motor_concat_varscaled.nii.gz\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "# Variance Normalisation - Run LR\n",
    "###########################################\n",
    "print(\"Normalizing variance for LR run\")\n",
    "\n",
    "# Calculate the variance for LR\n",
    "variance_LR = subprocess.check_output([\"fslstats\", functional_path_LR, \"-V\"]).decode().split()[1]\n",
    "print(f\"Global variance for LR: {variance_LR}\")\n",
    "\n",
    "# Compute the scaling factor (1 / sqrt(variance))\n",
    "scale_LR = 1 / (float(variance_LR) ** 0.5)\n",
    "print(f\"Scaling factor for LR: {scale_LR}\")\n",
    "\n",
    "# Apply scaling to normalize the variance\n",
    "scaled_LR_path = os.path.join(deriv_func, \"tfMRI_MOTOR_LR_varscaled.nii.gz\")\n",
    "subprocess.run([\"fslmaths\", functional_path_LR, \"-mul\", str(scale_LR), scaled_LR_path], check=True)\n",
    "print(f\"Saved normalized LR run: {scaled_LR_path}\\n\")\n",
    "\n",
    "# Delete intermediary variables to free memory\n",
    "del variance_LR, scale_LR\n",
    "gc.collect()\n",
    "\n",
    "###########################################\n",
    "# Variance Normalisation - Run RL\n",
    "###########################################\n",
    "print(\"Normalizing variance for RL run\")\n",
    "\n",
    "# Calculate the variance for RL\n",
    "variance_RL = subprocess.check_output([\"fslstats\", functional_path_RL, \"-V\"]).decode().split()[1]\n",
    "print(f\"Global variance for RL: {variance_RL}\")\n",
    "\n",
    "# Compute the scaling factor for RL (1 / sqrt(variance))\n",
    "scale_RL = 1 / (float(variance_RL) ** 0.5)\n",
    "print(f\"Scaling factor for RL: {scale_RL}\")\n",
    "\n",
    "# Apply scaling to normalize the variance\n",
    "scaled_RL_path = os.path.join(deriv_func, \"tfMRI_MOTOR_RL_varscaled.nii.gz\")\n",
    "subprocess.run([\"fslmaths\", functional_path_RL, \"-mul\", str(scale_RL), scaled_RL_path], check=True)\n",
    "print(f\"Saved normalized RL run: {scaled_RL_path}\\n\")\n",
    "\n",
    "# Delete intermediary variables to free memory\n",
    "del variance_RL, scale_RL\n",
    "gc.collect()\n",
    "\n",
    "###########################################\n",
    "# Concatenate the two runs\n",
    "###########################################\n",
    "print(\"Concatenating normalized runs...\")\n",
    "\n",
    "concat_path = os.path.join(deriv_func, f\"sub-{subject_id}_motor_concat_varscaled.nii.gz\")\n",
    "subprocess.run([\"fslmerge\", \"-t\", concat_path, scaled_LR_path, scaled_RL_path], check=True)\n",
    "\n",
    "print(f\"Done! Final concatenated file: {concat_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)b) Motion correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting the motion correction\")\n",
    "\n",
    "path_motioncorr_LR = op.join(deriv_func, \"tfMRI_MOTOR_LR_motioncorrected\")            # path to the motion corrected file\n",
    "path_motioncorr_RL = op.join(deriv_func, \"tfMRI_MOTOR_RL_motioncorrected\")            # path to the motion corrected file\n",
    "\n",
    "mcflirt(infile=functional_path_LR, o=path_motioncorr_LR, plots=True, report=True, dof=6, mats=True)\n",
    "print(\"Motor LR corrected\")\n",
    "mcflirt(infile=functional_path_RL, o=path_motioncorr_RL, plots=True, report=True, dof=6, mats=True)\n",
    "print(\"Motor RL corrected\")\n",
    "\n",
    "# Display result of LR\n",
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(functional_path_LR)\n",
    "fsleyesDisplay.load(path_motioncorr_LR)\n",
    "print(\"Motion corrected done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to run if save is not possible anymore ### ATTENTION SUPPRIME TOOOOUT LES PREPROCESS DATA!!!!!!!!!!!\n",
    " \n",
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# folder_to_clear = \"derivatives/preprocessed_data/subject_101410\"\n",
    "# if os.path.exists(folder_to_clear):\n",
    "#     shutil.rmtree(folder_to_clear)\n",
    "#     print(\"Dossier supprimé pour libérer de l'espace.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In order to determine how many in how many images the subject moves more than what is tolerated, we can run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lr_par_path = op.join(deriv_func, 'tfMRI_MOTOR_LR_motioncorrected.par')\n",
    "rl_par_path = op.join(deriv_func, 'tfMRI_MOTOR_RL_motioncorrected.par')\n",
    "\n",
    "def load_mot_params_fsl_6_dof(path):\n",
    "    return pd.read_csv(path, sep='  ', header=None, engine='python', names=['Rotation x', 'Rotation y', 'Rotation z','Translation x', 'Translation y', 'Translation z'])\n",
    "\n",
    "mot_params_LR = load_mot_params_fsl_6_dof(lr_par_path)\n",
    "mot_params_LR\n",
    "\n",
    "mot_params_RL = load_mot_params_fsl_6_dof(rl_par_path)\n",
    "mot_params_RL\n",
    "\n",
    "def compute_FD_power(mot_params):\n",
    "    framewise_diff = mot_params.diff().iloc[1:]\n",
    "\n",
    "    rotation_params = framewise_diff[['Rotation x', 'Rotation y', 'Rotation z']]\n",
    "    \n",
    "    convert_rot_to_arc = rotation_params*50                                    # transform from angle to arc of circle for a radius of 50mm, normal used values are 50 or 90 mm\n",
    "    translation_params = framewise_diff[['Translation x', 'Translation y', 'Translation z']]\n",
    "    fd = convert_rot_to_arc.abs().sum(axis=1) + translation_params.abs().sum(axis=1)\n",
    "    return fd\n",
    "\n",
    "fd_LR = compute_FD_power(mot_params_LR).to_numpy()\n",
    "threshold_LR = np.quantile(fd_LR,0.75) + 1.5*(np.quantile(fd_LR,0.75) - np.quantile(fd_LR,0.25))\n",
    "\n",
    "#%matplotlib inline\n",
    "plt.plot(list(range(1, fd_LR.size+1)), fd_LR)\n",
    "plt.xlabel('Volume')\n",
    "plt.ylabel('FD displacement (mm)')\n",
    "plt.hlines(threshold_LR, 0, 370,colors='black', linestyles='dashed', label='FD threshold')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Indices of frames of LR motion above threshold: {(np.where(fd_LR > threshold_LR)[0] + 1).tolist()}\")\n",
    "\n",
    "fd_RL = compute_FD_power(mot_params_RL).to_numpy()\n",
    "threshold_RL = np.quantile(fd_RL,0.75) + 1.5*(np.quantile(fd_RL,0.75) - np.quantile(fd_RL,0.25))\n",
    "\n",
    "plt.plot(list(range(1, fd_RL.size+1)), fd_RL)\n",
    "plt.xlabel('Volume')\n",
    "plt.ylabel('FD displacement (mm)')\n",
    "plt.hlines(threshold_RL, 0, 370,colors='black', linestyles='dashed', label='FD threshold')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Indices of frames of RL motion above threshold: {(np.where(fd_RL > threshold_RL)[0] + 1).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)c) Co-registration (bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions : (91, 109, 91, 568)\n",
      "(91, 109, 91)\n"
     ]
    }
   ],
   "source": [
    "betted_brain_path = op.join(deriv_anat, 'sub-{}_T1w'.format(subject_id))\n",
    "concat_path = os.path.join(deriv_func, f\"sub-{subject_id}_motor_concat_varscaled.nii.gz\")\n",
    "\n",
    "reference= betted_brain_path\n",
    "source= nib.load(concat_path)\n",
    "data_source = source.get_fdata()\n",
    "print(\"Dimensions :\", data_source.shape)\n",
    "\n",
    "volume_index = 10  # par exemple, le 11e volume\n",
    "volume_source = data_source[..., volume_index]\n",
    "print(volume_source.shape) \n",
    "\n",
    "#mni_template = op.expandvars(op.join('$FSLDIR', 'data', 'standard', 'MNI152_T1_1mm_brain'))\n",
    "#reference= mni_template\n",
    "\n",
    "spacing = tuple(source.header.get_zooms()[:3])\n",
    "origin = tuple(source.affine[:3, 3])\n",
    "moving_image = ants.from_numpy(volume_source, origin=origin, spacing=spacing)\n",
    "fixed_image = ants.image_read(reference + '.nii.gz')\n",
    "\n",
    "# Compute the transformation (non linear) to put align the moving image to the fixed image\n",
    "transformation = ants.registration(fixed=fixed_image, moving=moving_image, type_of_transform = 'SyN' )\n",
    "\n",
    "# After the transformation has been computed, apply it\n",
    "warpedImage = ants.apply_transforms(fixed=fixed_image, moving=moving_image, transformlist=transformation['fwdtransforms'])\n",
    "\n",
    "# Save the image to disk\n",
    "resultAnts = op.join(deriv_anat,'sub-{}_T1w_mni_SyN.nii.gz'.format(subject_id))\n",
    "ants.image_write(warpedImage, resultAnts)\n",
    "\n",
    "# Inspect the results with FSLeyes or freeview, as you prefer :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(reference) \n",
    "fsleyesDisplay.load(resultAnts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)d) Gaussian smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_path = concat_path\n",
    "cmd = 'fslmaths {} -s {} {}_smoothed-6mm'.format(output_path, 6/2.3548, output_path)\n",
    "subprocess.run(['fslmaths',output_path, '-s', str(6/2.3548), '{}_smoothed-6mm'.format(output_path)])\n",
    "results_smoothing = '{}_smoothed-6mm.nii.gz'.format(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(output_path) \n",
    "fsleyesDisplay.load(results_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.glm.first_level import make_first_level_design_matrix, FirstLevelModel\n",
    "from nilearn.plotting import plot_design_matrix\n",
    "from nilearn.plotting import plot_stat_map\n",
    "from IPython.display import display\n",
    "from nilearn.image import mean_img\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fMRI_path = '/data/subject101410/fMRI'\n",
    "\n",
    "func_path_LR = op.join(fMRI_path, 'tfMRI_MOTOR_LR') \n",
    "func_path_RL = op.join(fMRI_path, 'tfMRI_MOTOR_RL') \n",
    "\n",
    "events_LR = pd.read_csv(op.join(func_path_LR, 'events_LR.csv'), sep=',')\n",
    "events_RL = pd.read_csv(op.join(func_path_RL, 'events_RL.csv'), sep=',')\n",
    "\n",
    "print(\"Events Table — LR Run\")\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)\n",
    "display(events_LR.head(10))\n",
    "\n",
    "print(\"Events Table — RL Run\")\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)\n",
    "display(events_RL.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "img = nib.load(func_img)\n",
    "n_scans = img.shape[3]           # number of volumes\n",
    "TR = img.header.get_zooms()[3]   # repetition time\n",
    "frame_times = np.arange(n_scans) * TR  # time vector for each volume\n",
    "\n",
    "# design matrix LR\n",
    "img_LR = nib.load(func_img_LR) #Replace func_img_LR by the right file\n",
    "n_scans_LR = img_LR.shape[3]\n",
    "TR = 0.72\n",
    "frame_times_LR = np.arange(n_scans_LR) * TR\n",
    "\n",
    "design_matrix_LR = make_first_level_design_matrix(\n",
    "    frame_times_LR,\n",
    "    events_LR,\n",
    "    hrf_model='spm',\n",
    "    drift_model='cosine',\n",
    "    high_pass=0.01)\n",
    "\n",
    "print('Design Matrix LR')\n",
    "plot_design_matrix(fmri_glm_LR.design_matrices_[0])\n",
    "\n",
    "# design matrix RL\n",
    "img_RL = nib.load(func_img_RL) #Replace func_img_LR by the right file\n",
    "n_scans_RL = img_RL.shape[3]\n",
    "frame_times_RL = np.arange(n_scans_RL) * TR\n",
    "\n",
    "design_matrix_RL = make_first_level_design_matrix(\n",
    "    frame_times_RL,\n",
    "    events_RL,\n",
    "    hrf_model='spm',\n",
    "    drift_model='cosine',\n",
    "    high_pass=0.01)\n",
    "\n",
    "print( 'Design Matrix RL')\n",
    "plot_design_matrix(fmri_glm_RL.design_matrices_[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLM_fMRI = FirstLevelModel(t_r=0.72,\n",
    "                           noise_model='ar1',\n",
    "                           standardize=False,\n",
    "                           hrf_model='spm',\n",
    "                           drift_model=None,\n",
    "                           high_pass=.01)\n",
    "\n",
    "# Fit the model to our design and data\n",
    "fmri_glm_LR = GLM_fMRI.fit(fmri_img, events_LR)\n",
    "fmri_glm_RL = GLM_fMRI.fit(fmri_img, events_RL)\n",
    "\n",
    "# Get regressors\n",
    "regressors_LR = fmri_glm_LR.design_matrices_[0].columns.tolist()\n",
    "print(regressors_LR)\n",
    "regressors_RL = fmri_glm_LR.design_matrices_[0].columns.tolist()\n",
    "print(regressors_RL)\n",
    "\n",
    "# Compute and plot t-map for each regressor and run\n",
    "for reg in regressors_LR:\n",
    "    t_map = fmri_glm.compute_contrast(reg, output_type='stat')\n",
    "    plotting.plot_stat_map(\n",
    "        t_map,\n",
    "        threshold=3.0,\n",
    "        display_mode='ortho',\n",
    "        title=f'Statistical map: {reg}'\n",
    "    )\n",
    "\n",
    "for reg in regressors_RL:\n",
    "    t_map = fmri_glm_RL.compute_contrast(reg, output_type='stat')\n",
    "    plotting.plot_stat_map(\n",
    "        t_map,\n",
    "        threshold=3.0,\n",
    "        display_mode='ortho',\n",
    "        title=f'Statistical map: {reg}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments on t-map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_vector_hand_vs_foot = [1, 1, -1, -1, 0, 0] # depend on the regressor list, might be changed\n",
    "\n",
    "activation_map_LR = fmri_glm.compute_contrast(contrast_vector_hand_vs_foot,\n",
    "                                  output_type='effect_size')\n",
    "activation_map_RL = fmri_glm.compute_contrast(contrast_vector_hand_vs_foot,\n",
    "                                  output_type='effect_size')\n",
    "#nib.save(z_map, 'z_map_uncorrected.nii.gz')\n",
    "\n",
    "# activation map for LR\n",
    "plotting.plot_stat_map(\n",
    "    activation_map_LR,\n",
    "    threshold=0.5,          # might need to be adjusted as needed\n",
    "    display_mode='ortho',\n",
    "    cut_coords=(0, 0, 0),   # might need to be adjusted as needed\n",
    "    title='Hand > Foot Activation Map, LR'\n",
    "))\n",
    "\n",
    "# activation map for RL\n",
    "plotting.plot_stat_map(\n",
    "    activation_map_RL,\n",
    "    threshold=0.5,          # might need to be adjusted as needed\n",
    "    display_mode='ortho',\n",
    "    cut_coords=(0, 0, 0),   # might need to be adjusted as needed\n",
    "    title='Hand > Foot Activation Map, RL'\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#nib.save(activation_map, 'activation_map_hand_vs_foot.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# END OF OUR CODE, BELOW IS THE NEURODESK TEMPLATE (IGNORE BELOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datalad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "scroll-output"
    ],
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Install the dataset from GitHub using DataLad. \n",
    "# Replace the URL with the one corresponding to the OpenNeuro dataset you want to use.\n",
    "datalad install https://github.com/OpenNeuroDatasets/ds000102.git \n",
    "\n",
    "# Navigate into the dataset directory. \n",
    "# This should match the name of the dataset repository.\n",
    "cd ds000102\n",
    "\n",
    "# Optional: List available subjects\n",
    "ls -d sub-*\n",
    "\n",
    "# Download data for a specific subject (e.g., sub-08).\n",
    "# You can change the subject ID to the one you're interested in.\n",
    "datalad get sub-08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the OSF client to clone the contents of the OSF project with ID 'ru43c' into the current directory\n",
    "# The output is suppressed for cleaner display using > /dev/null 2>&1\n",
    "!osf -p ru43c clone . > /dev/null 2>&1\n",
    "\n",
    "# Extract the contents of the downloaded tar archive, which contains DICOM files\n",
    "!tar xf osfstorage/dicoms-unsorted.tar\n",
    "\n",
    "# Remove the now-unneeded 'osfstorage' directory to keep things tidy\n",
    "!rm -rf osfstorage/\n",
    "\n",
    "# Display the top of the folder tree for the extracted DICOM files (limited with `head` to avoid overwhelming output)\n",
    "!tree dicoms-unsorted | head\n",
    "\n",
    "# Print a count of how many unsorted DICOM files were extracted\n",
    "!echo -e \"...\\nThere are `ls dicoms-unsorted | wc -l` unsorted DICOMs in ./dicoms-unsorted/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of a dicom (one method for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to a DICOM file\n",
    "# You can change this to any other file in the dicoms-unsorted/ directory\n",
    "dcm_path = \"./dicoms-unsorted/MR.1.2.276.0.7230010.3.1.3.0.4480.1677042698.772469.84.dcm\"\n",
    "\n",
    "# Read the DICOM file into a Python object\n",
    "ds = pydicom.dcmread(dcm_path)\n",
    "\n",
    "# ----------------------------\n",
    "# Inspecting DICOM metadata (optional)\n",
    "# ----------------------------\n",
    "\n",
    "# Print a few key metadata fields (can be useful for QC or orientation)\n",
    "print(\"Patient ID:\", ds.get(\"PatientID\", \"N/A\"))\n",
    "print(\"Modality:\", ds.get(\"Modality\", \"N/A\"))\n",
    "print(\"Study Date:\", ds.get(\"StudyDate\", \"N/A\"))\n",
    "\n",
    "# ----------------------------\n",
    "# Visualizing the DICOM image\n",
    "# ----------------------------\n",
    "\n",
    "# Display the image stored in the DICOM file using matplotlib\n",
    "# Note: Some DICOM files (e.g., non-image series) may not contain pixel data\n",
    "plt.imshow(ds.pixel_array, cmap=plt.cm.gray) # Use grayscale color map\n",
    "plt.title(\"DICOM Slice: MR.1.1.dcm\") # Title of the plot\n",
    "plt.axis(\"off\") # Hide axis ticks/labels for cleaner view\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Be sure to annotate your example. You can also add Markdown blocs to provide explanations.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!bet ./ds000102/sub-08/anat/sub-08_T1w.nii.gz sub-08_T1w_brain.nii.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from ipyniivue import NiiVue\n",
    "nv = NiiVue()\n",
    "nv.load_volumes([{\"path\": \"sub-08_T1w_brain.nii.gz\"}])\n",
    "nv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies in Jupyter/Python\n",
    "- Using the package [watermark](https://github.com/rasbt/watermark) to document system environment and software versions used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "\n",
    "%watermark\n",
    "%watermark --iversions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
